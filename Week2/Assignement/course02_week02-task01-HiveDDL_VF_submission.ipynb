{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_create_external_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_create_external_table.hql\n",
    "-- Create external table posts_sample_external with suitable values\n",
    "\n",
    "-- adding necessary JARs and including database\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample_external;\n",
    "\n",
    "-- Create external table \n",
    "Create EXTERNAL TABLE posts_sample_external (\n",
    "                id STRING,\n",
    "                year STRING,\n",
    "                month STRING\n",
    ")\n",
    "-- Your code here\n",
    "ROW FORMAT\n",
    "    SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\n",
    "    WITH SERDEPROPERTIES (\n",
    "    \"input.regex\" = '^<row.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*).*'\n",
    ")\n",
    "LOCATION '/data/stackexchange1000/posts';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_create_managed_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_create_managed_table.hql\n",
    "-- create managed table\n",
    "-- Check that this table contains info about year and month\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "\n",
    "CREATE TABLE posts_sample (\n",
    "             id STRING\n",
    ") \n",
    "PARTITIONED BY (year STRING, month STRING);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_insert_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_insert_table.hql\n",
    "\n",
    "SET hive.exec.dynamic.partition=true;\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "SET hive.exec.max.dynamic.partitions=500;\n",
    "SET hive.exec.max.dynamic.partitions.pernode=256;\n",
    "SET hive.error.on.empty.partition=true;\n",
    "\n",
    "-- adding necessary JARs and including database\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "-- Insert data to the managed table\n",
    "\n",
    "USE demodb;\n",
    "-- filling managed posts table from external one\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "\n",
    "FROM posts_sample_external\n",
    "INSERT OVERWRITE TABLE posts_sample\n",
    "PARTITION (year, month)\n",
    "SELECT id, year, month\n",
    "WHERE year IS NOT NULL and month is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_result.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_result.hql\n",
    "-- Your code here\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "\n",
    "select concat_ws(\"\\t\", year, concat_ws(\"-\", year, month), row_cnt)\n",
    "from(\n",
    "SELECT year, month,STRING(count(*)) as row_cnt, ROW_NUMBER() OVER (ORDER BY year, month) as row_nbr\n",
    "FROM posts_sample\n",
    "GROUP BY year, month\n",
    ") as temp\n",
    "where row_nbr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat task1_create_external_table.hql > task1.hql\n",
    "!cat task1_create_managed_table.hql >> task1.hql\n",
    "!cat task1_insert_table.hql >> task1.hql\n",
    "!cat task1_result.hql >> task1.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Create external table posts_sample_external with suitable values\r\n",
      "\r\n",
      "-- adding necessary JARs and including database\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "DROP TABLE IF EXISTS posts_sample_external;\r\n",
      "\r\n",
      "-- Create external table \r\n",
      "Create EXTERNAL TABLE posts_sample_external (\r\n",
      "                id STRING,\r\n",
      "                year STRING,\r\n",
      "                month STRING\r\n",
      ")\r\n",
      "-- Your code here\r\n",
      "ROW FORMAT\r\n",
      "    SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\r\n",
      "    WITH SERDEPROPERTIES (\r\n",
      "    \"input.regex\" = '^<row.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*).*'\r\n",
      ")\r\n",
      "LOCATION '/data/stackexchange1000/posts';\r\n",
      "-- create managed table\r\n",
      "-- Check that this table contains info about year and month\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "DROP TABLE IF EXISTS posts_sample;\r\n",
      "\r\n",
      "CREATE TABLE posts_sample (\r\n",
      "             id STRING\r\n",
      ") \r\n",
      "PARTITIONED BY (year STRING, month STRING);\r\n",
      "\r\n",
      "SET hive.exec.dynamic.partition=true;\r\n",
      "SET hive.exec.dynamic.partition.mode=nonstrict;\r\n",
      "SET hive.exec.max.dynamic.partitions=500;\r\n",
      "SET hive.exec.max.dynamic.partitions.pernode=256;\r\n",
      "SET hive.error.on.empty.partition=true;\r\n",
      "\r\n",
      "-- adding necessary JARs and including database\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\r\n",
      "\r\n",
      "-- Insert data to the managed table\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "-- filling managed posts table from external one\r\n",
      "SET hive.exec.dynamic.partition.mode=nonstrict;\r\n",
      "\r\n",
      "FROM posts_sample_external\r\n",
      "INSERT OVERWRITE TABLE posts_sample\r\n",
      "PARTITION (year, month)\r\n",
      "SELECT id, year, month\r\n",
      "WHERE year IS NOT NULL and month is not NULL;\r\n",
      "-- Your code here\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "\r\n",
      "select concat_ws(\"\\t\", year, concat_ws(\"-\", year, month), row_cnt)\r\n",
      "from(\r\n",
      "SELECT year, month,STRING(count(*)) as row_cnt, ROW_NUMBER() OVER (ORDER BY year, month) as row_nbr\r\n",
      "FROM posts_sample\r\n",
      "GROUP BY year, month\r\n",
      ") as temp\r\n",
      "where row_nbr = 3\r\n"
     ]
    }
   ],
   "source": [
    "!cat task1.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$(document).ready(function() {\n",
       "    console.log('Ready');\n",
       "    \n",
       "    \n",
       "    function is_hive_command(list_tokens) {\n",
       "        return list_tokens.indexOf('hive') > -1 && \n",
       "             list_tokens.indexOf('f') > -1 &&\n",
       "             list_tokens.indexOf('-') > -1 && \n",
       "             list_tokens.indexOf('!') > -1 &&\n",
       "             list_tokens.indexOf('hql') > -1 && \n",
       "             list_tokens.indexOf('writefile') == -1;\n",
       "    } \n",
       "    \n",
       "    function collectText(input_tag) {\n",
       "\n",
       "        var result_string = [];\n",
       "        $.each($(input_tag).children(), function(index, child) {\n",
       "            result_string.push($(child).text());\n",
       "        });\n",
       "        return [result_string, is_hive_command(result_string)];\n",
       "    };\n",
       "    \n",
       "    var filtered_results = $(\".cell.code_cell.rendered\").filter(function(index, element) {\n",
       "        var out = collectText($(element).find('.CodeMirror-line').find('span'));\n",
       "        console.log(out);\n",
       "        return collectText($(element).find('.CodeMirror-line').find('span'))[1];\n",
       "    });\n",
       "    $(filtered_results).remove();\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$(document).ready(function() {\n",
    "    console.log('Ready');\n",
    "    \n",
    "    \n",
    "    function is_hive_command(list_tokens) {\n",
    "        return list_tokens.indexOf('hive') > -1 && \n",
    "             list_tokens.indexOf('f') > -1 &&\n",
    "             list_tokens.indexOf('-') > -1 && \n",
    "             list_tokens.indexOf('!') > -1 &&\n",
    "             list_tokens.indexOf('hql') > -1 && \n",
    "             list_tokens.indexOf('writefile') == -1;\n",
    "    } \n",
    "    \n",
    "    function collectText(input_tag) {\n",
    "\n",
    "        var result_string = [];\n",
    "        $.each($(input_tag).children(), function(index, child) {\n",
    "            result_string.push($(child).text());\n",
    "        });\n",
    "        return [result_string, is_hive_command(result_string)];\n",
    "    };\n",
    "    \n",
    "    var filtered_results = $(\".cell.code_cell.rendered\").filter(function(index, element) {\n",
    "        var out = collectText($(element).find('.CodeMirror-line').find('span'));\n",
    "        console.log(out);\n",
    "        return collectText($(element).find('.CodeMirror-line').find('span'))[1];\n",
    "    });\n",
    "    $(filtered_results).remove();\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008\t2008-10\t73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar]\n",
      "OK\n",
      "Time taken: 0.712 seconds\n",
      "OK\n",
      "Time taken: 0.225 seconds\n",
      "OK\n",
      "Time taken: 0.144 seconds\n",
      "OK\n",
      "Time taken: 0.027 seconds\n",
      "OK\n",
      "Time taken: 0.129 seconds\n",
      "OK\n",
      "Time taken: 0.13 seconds\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar]\n",
      "OK\n",
      "Time taken: 0.023 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = jovyan_20201216001935_e030d388-8f38-42ac-a87a-b5f9dc213c6e\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1608075810918_0016, Tracking URL = http://172.17.0.31:8088/proxy/application_1608075810918_0016/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1608075810918_0016\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2020-12-16 00:19:42,222 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-12-16 00:19:57,518 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 11.06 sec\n",
      "MapReduce Total cumulative CPU time: 11 seconds 60 msec\n",
      "Ended Job = job_1608075810918_0016\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://localhost:9000/user/jovyan/demodb/posts_sample/.hive-staging_hive_2020-12-16_00-19-35_733_4363005533085326338-1/-ext-10000\n",
      "Loading data to table demodb.posts_sample partition (year=null, month=null)\n",
      "\n",
      "\n",
      "chmod: File hdfs://localhost:9000/user/jovyan/demodb/posts_sample/year=2010/month=04 does not exist.\n",
      "\t Time taken to load dynamic partitions: 4.082 seconds\n",
      "\t Time taken for adding to write entity : 0.007 seconds\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 11.06 sec   HDFS Read: 60004088 HDFS Write: 347300 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 11 seconds 60 msec\n",
      "OK\n",
      "Time taken: 28.67 seconds\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.028 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = jovyan_20201216002004_0292ff86-fcd2-4234-a32f-63fb53412517\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1608075810918_0017, Tracking URL = http://172.17.0.31:8088/proxy/application_1608075810918_0017/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1608075810918_0017\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-12-16 00:20:10,549 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-12-16 00:20:14,630 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.9 sec\n",
      "2020-12-16 00:20:19,708 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.99 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 990 msec\n",
      "Ended Job = job_1608075810918_0017\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1608075810918_0018, Tracking URL = http://172.17.0.31:8088/proxy/application_1608075810918_0018/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1608075810918_0018\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-12-16 00:20:30,333 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-12-16 00:20:34,400 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.05 sec\n",
      "2020-12-16 00:20:39,472 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.83 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 830 msec\n",
      "Ended Job = job_1608075810918_0018\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.99 sec   HDFS Read: 386332 HDFS Write: 2903 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.83 sec   HDFS Read: 11960 HDFS Write: 115 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 820 msec\n",
      "OK\n",
      "Time taken: 36.109 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hive -f task1.hql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
