{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common warnings:\n",
    "\n",
    "1. __Backup your solution into the 'work' directory inside the home directory ('/home/jovyan'). It is the only one that state will be saved over sessions.__\n",
    "\n",
    "1. Please, ensure that you call the right interpreter (python2 or python3). Do not write just \"python\" without the major version. There is no guarantee that any particular version of Python is set as the default one in the Grading system.\n",
    "\n",
    "1. One cell must contain only one programming language.\n",
    "E.g. if a cell contains Python code and you also want to call a bash-command (using “!”) in it, you should move the bash to another cell.\n",
    "\n",
    "1. Our IPython converter is an improved version of the standard converter Nbconvert and it can process most of Jupyter's magic commands correctly (e.g. it understands \"%%bash\" and executes the cell as a \"bash\"-script). However, we highly recommend to avoid magics wherever possible.\n",
    "\n",
    "#### Hive specific warning:\n",
    "\n",
    "__A Hive task notebook works only in the home directory!__ (The parent directory for starter_and_demos)\n",
    "Do not submit `CREATE/DROP DATABASE` line(s) into the Grading system. Most likely you will get \"Permission denied\" error if such line is submitted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The demonstrative notebook for Hive assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run any HiveQL query in the notebook you should:\n",
    "1. write the code of query into a separate file using `%%writefile [-a] <file>` magic,\n",
    "2. execute this file in hive using `! hive -f <file>` command.\n",
    "\n",
    "To make grading system check a task correctly, execution command must be in a separate cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creation the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, create your Hive database. You can name the database whatever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop database if it has already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "# %%writefile creation_db.hql\n",
    "\n",
    "# DROP DATABASE IF EXISTS demodb CASCADE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "# %%writefile -a creation_db.hql\n",
    "# CREATE DATABASE demodb LOCATION '/user/jovyan/demodb';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the file we filled earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 3.871 seconds\n",
      "OK\n",
      "Time taken: 0.226 seconds\n"
     ]
    }
   ],
   "source": [
    "# ! hive -f creation_db.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the real Hadoop-cluster where your submission will be checked we already have precreated Hive databases for all users. This helps to avoid database name conflicts. If you're the new user, the database will be created during your first submission of Hive assignment. The system won't allow you to create your own database on Hadoop-cluster so when you submit the final version of the task you shoud **remove or comment** all the lines related to database's dropping and creation. \n",
    "\n",
    "You can left all the lines with `USE` without any changes. The grading system will replace database's name to name of the precreated database. In assignments 2 and 3 you'll need to use `stackoverflow_` database. This database's name will not be changed by the grading system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation the external table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us our source dataset have 2 collumns:\n",
    "* ip-address,\n",
    "* its subnet's mask.\n",
    "\n",
    "For example:\n",
    "```\n",
    "148.45.113.216\t255.255.255.248\n",
    "203.98.141.0\t255.255.255.240\n",
    "183.168.36.0\t255.255.255.128\n",
    "111.157.172.232\t255.255.255.248\n",
    "80.46.87.0\t255.255.255.0\n",
    "247.248.233.0\t255.255.255.128\n",
    "```\n",
    "Now we'll create the external table with 2 fields: ip and mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exteral_table.hql\n"
     ]
    }
   ],
   "source": [
    "# %%writefile exteral_table.hql\n",
    "\n",
    "# ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "# USE demodb;\n",
    "# DROP TABLE IF EXISTS Subnets;\n",
    "\n",
    "# CREATE EXTERNAL TABLE Subnets (\n",
    "#     ip STRING,\n",
    "#     mask STRING\n",
    "# )\n",
    "# ROW FORMAT DELIMITED FIELDS TERMINATED BY  '\\t'\n",
    "# STORED AS TEXTFILE\n",
    "# LOCATION '/data/subnets/ips';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.683 seconds\n",
      "OK\n",
      "Time taken: 0.848 seconds\n",
      "OK\n",
      "Time taken: 0.18 seconds\n"
     ]
    }
   ],
   "source": [
    "# ! hive -f exteral_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo query on created table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a simpe query:\n",
    " > Compute avarage value of IPs for each subnet's mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "USE demodb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a query.hql\n",
    "\n",
    "SELECT AVG(counts.cnt)\n",
    "FROM (\n",
    "    SELECT mask, count(ip) as cnt\n",
    "    FROM Subnets\n",
    "    GROUP BY mask\n",
    ") counts;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take into account that the grading system catch all output (both result and MapReduce logs) from the last cell of the notebook, so __don't__ redirect any output from this cell to `/dev/null`\n",
    "\n",
    "#### Final notice:\n",
    "\n",
    "1. Please take into account that you must __not__ redirect __stderr__ to anywhere. Hadoop, Hive, and Spark print their logs to stderr and the Grading system also reads and analyses it.\n",
    "\n",
    "1. During checking the code from the notebook, the system runs all notebook's cells and reads the output of only the last filled cell. It is clear that any exception should not be thrown in the running cells. If you decide to write some text in a cell, you should change the style of the cell to Markdown (Cell -> Cell type -> Markdown).\n",
    "\n",
    "1. The Grader takes into account the output from the sample dataset you have in the notebook. Therefore, you have to \"Run All\" cells in the notebook before you send the ipynb solution.\n",
    "\n",
    "1. The name of the notebook must contain only Roman letters, numbers and characters “-” or “_”. For example, Windows adds something like \" (2)\" (with the leading space) at the end of a filename if you try to download a file with the same name. This is a problem, because you will have a space character and curly braces \"(\" and \")\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.72 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = jovyan_20201214183854_af206af2-be00-4d3d-bddd-36d61696e014\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1607970259371_0001, Tracking URL = http://172.17.0.22:8088/proxy/application_1607970259371_0001/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1607970259371_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-12-14 18:39:04,091 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-12-14 18:39:08,206 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec\n",
      "2020-12-14 18:39:13,313 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.17 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 170 msec\n",
      "Ended Job = job_1607970259371_0001\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1607970259371_0002, Tracking URL = http://172.17.0.22:8088/proxy/application_1607970259371_0002/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1607970259371_0002\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-12-14 18:39:24,605 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-12-14 18:39:28,693 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.91 sec\n",
      "2020-12-14 18:39:33,792 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.97 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 970 msec\n",
      "Ended Job = job_1607970259371_0002\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.17 sec   HDFS Read: 15209 HDFS Write: 127 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 1.97 sec   HDFS Read: 6056 HDFS Write: 118 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 4 seconds 140 msec\n",
      "OK\n",
      "35.714285714285715\n",
      "Time taken: 40.132 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query.hql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
