{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common warnings:\n",
    "\n",
    "1. __Backup your solution into the 'work' directory inside the home directory ('/home/jovyan'). It is the only one that state will be saved over sessions.__\n",
    "\n",
    "1. Please, ensure that you call the right interpreter (python2 or python3). Do not write just \"python\" without the major version. There is no guarantee that any particular version of Python is set as the default one in the Grading system.\n",
    "\n",
    "1. One cell must contain only one programming language.\n",
    "E.g. if a cell contains Python code and you also want to call a bash-command (using “!”) in it, you should move the bash to another cell.\n",
    "\n",
    "1. Our IPython converter is an improved version of the standard converter Nbconvert and it can process most of Jupyter's magic commands correctly (e.g. it understands \"%%bash\" and executes the cell as a \"bash\"-script). However, we highly recommend to avoid magics wherever possible.\n",
    "\n",
    "\n",
    "#### Hive specific warning:\n",
    "\n",
    "__A Hive task notebook works only in the home directory!__ (The parent directory for starter_and_demos)\n",
    "Do not submit `CREATE/DROP DATABASE` line(s) into the Grading system. Most likely you will get \"Permission denied\" error if such line is submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to create an external table on the posts data of the `stackoverflow.com` website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Intro. Creation of the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the sandbox database where you will complete your assignment.\n",
    "\n",
    "<b>Note!</b> This code shouldn't be in your submission. Please, remove this code from the notebook before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile creation_db.hql\n",
    "DROP DATABASE IF EXISTS demodb CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a creation_db.hql\n",
    "CREATE DATABASE demodb LOCATION '/user/jovyan/demodb';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to remove this code before submission!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Exploration of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have created the database. Let's create your own table for users and posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's watch at the datasets for `users` which are located at `/data/stackexchange1000/posts` and for `posts` which is located at `/data/stackexchange1000/users`. Print the first three rows of those datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:status:Reading \t\n",
      "<row Id=\"1394\" PostTypeId=\"2\" ParentId=\"1390\" CreationDate=\"2008-08-04T16:38:03.667\" Score=\"16\" Body=\"&lt;p&gt;Not sure how credible &lt;a href=&quot;http://www.builderau.com.au/program/windows/soa/Getting-started-with-Windows-Server-2008-Core-edition/0,339024644,339288700,00.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;this source is&lt;/a&gt;, but:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The Windows Server 2008 Core edition can:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the file server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Hyper-V virtualization server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Directory Services role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DHCP server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the IIS Web server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DNS server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Active Directory Lightweight Directory Services.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the print server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;  &#xA;  &lt;p&gt;The Windows Server 2008 Core edition cannot:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run a SQL Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run an Exchange Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Internet Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Windows Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Host a remote desktop session.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run MMC snap-in consoles locally.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;\" OwnerUserId=\"91\" LastEditorUserId=\"1\" LastEditorDisplayName=\"Jeff Atwood\" LastEditDate=\"2008-08-27T13:02:50.273\" LastActivityDate=\"2008-08-27T13:02:50.273\" CommentCount=\"1\" />\t\n",
      "<row Id=\"3543\" PostTypeId=\"2\" ParentId=\"3530\" CreationDate=\"2008-08-06T15:24:00.787\" Score=\"37\" Body=\"&lt;p&gt;from &lt;a href=&quot;http://www.timocracy.com/articles/2008/02/21/calling-invoking-rails-rake-tasks-from-within-ruby-for-testing-try-2&quot; rel=&quot;nofollow noreferrer&quot;&gt;timocracy.com&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;require 'rake'&#xA;require 'rake/rdoctask'&#xA;require 'rake/testtask'&#xA;require 'tasks/rails'&#xA;&#xA;def capture_stdout&#xA;  s = StringIO.new&#xA;  oldstdout = $stdout&#xA;  $stdout = s&#xA;  yield&#xA;  s.string&#xA;ensure&#xA;  $stdout = oldstdout&#xA;end&#xA;&#xA;Rake.application.rake_require '../../lib/tasks/metric_fetcher'&#xA;results = capture_stdout {Rake.application['metric_fetcher'].invoke}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\" OwnerUserId=\"399\" LastActivityDate=\"2008-08-06T15:24:00.787\" CommentCount=\"3\" />\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -cat /data/stackexchange1000/posts/part-00000 | head -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:status:Reading \t\n",
      "<row Id=\"756\" Reputation=\"2358\" CreationDate=\"2008-08-08T15:31:50.013\" DisplayName=\"Simon Gillbee\" LastAccessDate=\"2016-12-09T15:38:03.453\" WebsiteUrl=\"http://simon.gillbee.com\" Location=\"Pearland, TX\" AboutMe=\"&lt;p&gt;Personally, I am a husband, step-father, grandfather, Christ-worshiper, singer, worship leader, computer programmer, reader, game player, kite flyer, generally all around fun guy (or that fungi?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Professionally, I've been developing both commercial and proprietary systems for the last 20 years. These days I'm primarily writing enterprise-scale client and server software using .NET and loving it!&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;SOreadytohelp&lt;/h1&gt;&#xA;\" Views=\"478\" UpVotes=\"352\" DownVotes=\"25\" Age=\"45\" AccountId=\"587\" />\t\n",
      "<row Id=\"2050\" Reputation=\"4177\" CreationDate=\"2008-08-20T00:32:49.217\" DisplayName=\"Eric Platon\" LastAccessDate=\"2016-12-10T22:24:27.217\" WebsiteUrl=\"\" Location=\"Tokyo, Japan\" AboutMe=\"\" Views=\"387\" UpVotes=\"1045\" DownVotes=\"6\" Age=\"46\" AccountId=\"1533\" />\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -cat /data/stackexchange1000/users/part-00000 | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, those rows contain some information about posts and users in XML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question.</b> Which fields for users and posts do you think are the most important for the analysis? And for joining tables? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Please, check your answer with this information!</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the lines not started with the \"row\" tags should be ignored. The valid row contains the following fields and their order is not defined:\n",
    "\n",
    "* Id (integer) - id of the post\n",
    "* PostTypeId (integer: 1 or 2) - 1 for questions, 2 for answers\n",
    "* CreationDate (date) - post creation date in the format \"YYYY-MM-DDTHH:MM:SS.ms\"\n",
    "* Tags (string, optional) - list of post tags, each tag is wrapped with html entities `&lt;` and `&gt;`\n",
    "* OwnerUserId (integer, optional) - user id of the post's author\n",
    "* ParentId (integer, optional) - for answers - id of the question\n",
    "* Score (integer) - score (votes) of a question or an answer, can be negative (!)\n",
    "* FavoriteCount (integer, optional) - how many times the question was added in the favorites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of the dataset contains StackOverflow users.\n",
    "\n",
    "The fields are the following and their order is also not defined:\n",
    "\n",
    "* Id (integer) - user id\n",
    "* Reputation (integer) - user's reputation\n",
    "* CreationDate (string) - creation date in the format \"YYYY-MM-DDTHH:MM:SS.ms\"\n",
    "* DisplayName (string) - user's name\n",
    "* Location (string, options) - user's country\n",
    "* Age (integer, optional) - user's age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train your regexp skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you will find out how to parse information for complex rows! You will try to create some examples for parsing! There are some general rules for parsing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To create a regular expression, which describes strings containing two patterns, where the order of the patterns is not defined use the following so-called ‘positive lookahead assertion’ with `?=` group modifier. For example, both strings “Washington Irving” and “Irving Washington” match the pattern:\n",
    "```\n",
    "(?=.*Washington)(?=.*Irving)\n",
    "```.\n",
    "2. To capture groups use round brackets. So, the pattern: `(?=.*(Washington))(?=.*(Irving))` captures `Washington` and `Irving` from both strings: \"William Arthur Irving Washington was an English first-class Cricketer\" and: “Washington Irving was an American writer”.\n",
    "3. Use `\\b` to specify boundaries of words and increase accuracy of your pattern. For example: pattern `(?=.*\\bID=(\\d+))(?=.*\\bUserID=(\\d+))` captures `1` and `2` from the string `ID=1 UserID=2`, whereas pattern without `\\b`: `(?=.*ID=(\\d+))(?=.*UserID=(\\d+))` returns the wrong groups: `2` and `2`.\n",
    "4. In Hive pattern for the external table in SERDEPROPERTIES `input.regex` should describe the whole input string, add `.*` at the end of the pattern.\n",
    "5. Don't forget that for the beginning of string should also be covered. That's why use the pattern `.*?` for lazy initialization of future patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, you can create your first regex for parsing Id from posts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question!</b> What will be your first regex for parsing Id from the posts? Don't forget to add steps 4 and 5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" href=\"#collapse-answer\">Check your answer!</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-answer\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">The correct answer is `\".*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*\"` </div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the first external table with one row which contains only `Id` field. Let's name it `posts_external_only_id`.\n",
    "You can watch the lecture for the SerDe format: <a href=\"https://www.coursera.org/learn/big-data-analysis/lecture/wAGe6/hive-analytics-regexserde-views\">Serde Format</a> and creation of external table in the Hive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo_example.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_example.hql\n",
    "\n",
    "-- adding necessary JARs and including database\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_external_only_id;\n",
    "\n",
    "-- Create external table \n",
    "Create EXTERNAL TABLE posts_external_only_id (\n",
    "                id STRING\n",
    ")\n",
    "-- Your code here\n",
    "ROW FORMAT\n",
    "    SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\n",
    "    WITH SERDEPROPERTIES (\n",
    "    \"input.regex\" = '.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*'\n",
    ")\n",
    "LOCATION '/data/stackexchange1000/posts';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! You have created your first table. Let us watch for this table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting describe.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile describe.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DESCRIBE posts_external_only_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that the data is  correctly parsed. For this case, take a select query that chooses for us first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_first_select.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_first_select.hql\n",
    "USE demodb;\n",
    "SELECT * FROM posts_external_only_id LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many posts are there in the dataset? Don't forget to clear the `NULL` values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting how_many_posts.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile how_many_posts.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "SELECT COUNT(*)\n",
    "FROM posts_external_only_id\n",
    "WHERE id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to parse different fields: for example, day and month of creation date. Don't forget that Hive will accept values for the capturing group in the lookahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to complete your task! Before this you can check your regular expression for parsing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_ROW = '<row Id=\"1394\" PostTypeId=\"2\" ParentId=\"1390\" CreationDate=\"2008-08-04T16:38:03.667\" Score=\"16\" Body=\"&lt;p&gt;Not sure how credible &lt;a href=&quot;http://www.builderau.com.au/program/windows/soa/Getting-started-with-Windows-Server-2008-Core-edition/0,339024644,339288700,00.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;this source is&lt;/a&gt;, but:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The Windows Server 2008 Core edition can:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the file server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Hyper-V virtualization server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Directory Services role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DHCP server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the IIS Web server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DNS server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Active Directory Lightweight Directory Services.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the print server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;  &#xA;  &lt;p&gt;The Windows Server 2008 Core edition cannot:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run a SQL Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run an Exchange Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Internet Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Windows Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Host a remote desktop session.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run MMC snap-in consoles locally.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;\" OwnerUserId=\"91\" LastEditorUserId=\"1\" LastEditorDisplayName=\"Jeff Atwood\" LastEditDate=\"2008-08-27T13:02:50.273\" LastActivityDate=\"2008-08-27T13:02:50.273\" CommentCount=\"1\" />'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_REGEX = '^<row\\s?(?=Id=\\\"(\\d+)).*?(?=CreationDate=\\\"(\\d+)-(\\d+)).*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.match(CHECK_REGEX, CHECK_ROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert result.group(0) == CHECK_ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1394', '2008', '08')\n"
     ]
    }
   ],
   "source": [
    "# Check that your groups are correct\n",
    "print(result.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Complete the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_create_external_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_create_external_table.hql\n",
    "-- Create external table posts_sample_external with suitable values\n",
    "\n",
    "-- adding necessary JARs and including database\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample_external;\n",
    "\n",
    "-- Create external table \n",
    "Create EXTERNAL TABLE posts_sample_external (\n",
    "                id STRING,\n",
    "                year STRING,\n",
    "                month STRING\n",
    ")\n",
    "-- Your code here\n",
    "ROW FORMAT\n",
    "    SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\n",
    "    WITH SERDEPROPERTIES (\n",
    "    \"input.regex\" = '^<row.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*).*'\n",
    ")\n",
    "LOCATION '/data/stackexchange1000/posts';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you have created your table correctly. Select the first 10 posts from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_check_select.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_check_select.hql\n",
    "\n",
    "-- Write select query for the first 10 rows\n",
    "\n",
    "USE demodb;\n",
    "SELECT *\n",
    "FROM posts_sample_external\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create managed table `posts_sample`. Create the partition by the month and by the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_create_managed_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_create_managed_table.hql\n",
    "-- create managed table\n",
    "-- Check that this table contains info about year and month\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "\n",
    "CREATE TABLE posts_sample (\n",
    "             id STRING\n",
    ") \n",
    "PARTITIONED BY (year STRING, month STRING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate data from the table `posts_sample_external` to the table `posts_sample`. Don't forget about the partitioning rules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_insert_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_insert_table.hql\n",
    "\n",
    "SET hive.exec.dynamic.partition=true;\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "SET hive.exec.max.dynamic.partitions=500;\n",
    "SET hive.exec.max.dynamic.partitions.pernode=256;\n",
    "SET hive.error.on.empty.partition=true;\n",
    "\n",
    "-- adding necessary JARs and including database\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "-- Insert data to the managed table\n",
    "\n",
    "USE demodb;\n",
    "-- filling managed posts table from external one\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "\n",
    "FROM posts_sample_external\n",
    "INSERT OVERWRITE TABLE posts_sample\n",
    "PARTITION (year, month)\n",
    "SELECT id, year, month\n",
    "WHERE year IS NOT NULL and month is not NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your table contains appropriate data about posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_watch_new_table.hql\n"
     ]
    }
   ],
   "source": [
    "# %%writefile task1_watch_new_table.hql\n",
    "# USE demodb;\n",
    "# SELECT * FROM posts_sample LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the third row of the dataset in the ascending order for the posts (firstly by year, after that by month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_result.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_result.hql\n",
    "-- Your code here\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "\n",
    "select concat_ws(\"\\t\", year, concat_ws(\"-\", year, month), row_cnt)\n",
    "from(\n",
    "SELECT year, month,STRING(count(*)) as row_cnt, ROW_NUMBER() OVER (ORDER BY year, month) as row_nbr\n",
    "FROM posts_sample\n",
    "GROUP BY year, month\n",
    ") as temp\n",
    "where row_nbr = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Submission part. Do not touch!! And simple run all cells below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your notebook from the steps <a href=\"#Step-4.-Complete-the-assignment\">Step 4</a> and <a href=\"#Step-5.-Submission-part.-Do-not-touch!!-And-simple-run-all-cells-below!\">Step 5</a> to the new notebook. Run all the cells! And submit the copied notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "76fguZmgJ5uD"
   },
   "outputs": [],
   "source": [
    "!cat task1_create_external_table.hql > task1.hql\n",
    "!cat task1_create_managed_table.hql >> task1.hql\n",
    "!cat task1_insert_table.hql >> task1.hql\n",
    "!cat task1_result.hql >> task1.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KK3zjw7EJ5uF"
   },
   "source": [
    "Take a look at your submission query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Create external table posts_sample_external with suitable values\r\n",
      "\r\n",
      "-- adding necessary JARs and including database\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "DROP TABLE IF EXISTS posts_sample_external;\r\n",
      "\r\n",
      "-- Create external table \r\n",
      "Create EXTERNAL TABLE posts_sample_external (\r\n",
      "                id STRING,\r\n",
      "                year STRING,\r\n",
      "                month STRING\r\n",
      ")\r\n",
      "-- Your code here\r\n",
      "ROW FORMAT\r\n",
      "    SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\r\n",
      "    WITH SERDEPROPERTIES (\r\n",
      "    \"input.regex\" = '^<row.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*).*'\r\n",
      ")\r\n",
      "LOCATION '/data/stackexchange1000/posts';\r\n",
      "-- create managed table\r\n",
      "-- Check that this table contains info about year and month\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "DROP TABLE IF EXISTS posts_sample;\r\n",
      "\r\n",
      "CREATE TABLE posts_sample (\r\n",
      "             id STRING\r\n",
      ") \r\n",
      "PARTITIONED BY (year STRING, month STRING);\r\n",
      "\r\n",
      "DESCRIBE posts_sample;\r\n",
      "\r\n",
      "SET hive.exec.dynamic.partition=true;\r\n",
      "SET hive.exec.dynamic.partition.mode=nonstrict;\r\n",
      "SET hive.exec.max.dynamic.partitions=500;\r\n",
      "SET hive.exec.max.dynamic.partitions.pernode=256;\r\n",
      "SET hive.error.on.empty.partition=true;\r\n",
      "\r\n",
      "-- adding necessary JARs and including database\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\r\n",
      "\r\n",
      "-- Insert data to the managed table\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "-- filling managed posts table from external one\r\n",
      "SET hive.exec.dynamic.partition.mode=nonstrict;\r\n",
      "\r\n",
      "FROM posts_sample_external\r\n",
      "INSERT OVERWRITE TABLE posts_sample\r\n",
      "PARTITION (year, month)\r\n",
      "SELECT id, year, month\r\n",
      "WHERE year IS NOT NULL and month is not NULL;\r\n",
      "-- Your code here\r\n",
      "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\r\n",
      "\r\n",
      "USE demodb;\r\n",
      "\r\n",
      "select concat_ws(\"\\t\", year, concat_ws(\"-\", year, month), row_cnt)\r\n",
      "from(\r\n",
      "SELECT year, month,STRING(count(*)) as row_cnt, ROW_NUMBER() OVER (ORDER BY year, month) as row_nbr\r\n",
      "FROM posts_sample\r\n",
      "GROUP BY year, month\r\n",
      ") as temp\r\n",
      "where row_nbr = 3\r\n"
     ]
    }
   ],
   "source": [
    "!cat task1.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$(document).ready(function() {\n",
       "    console.log('Ready');\n",
       "    \n",
       "    \n",
       "    function is_hive_command(list_tokens) {\n",
       "        return list_tokens.indexOf('hive') > -1 && \n",
       "             list_tokens.indexOf('f') > -1 &&\n",
       "             list_tokens.indexOf('-') > -1 && \n",
       "             list_tokens.indexOf('!') > -1 &&\n",
       "             list_tokens.indexOf('hql') > -1 && \n",
       "             list_tokens.indexOf('writefile') == -1;\n",
       "    } \n",
       "    \n",
       "    function collectText(input_tag) {\n",
       "\n",
       "        var result_string = [];\n",
       "        $.each($(input_tag).children(), function(index, child) {\n",
       "            result_string.push($(child).text());\n",
       "        });\n",
       "        return [result_string, is_hive_command(result_string)];\n",
       "    };\n",
       "    \n",
       "    var filtered_results = $(\".cell.code_cell.rendered\").filter(function(index, element) {\n",
       "        var out = collectText($(element).find('.CodeMirror-line').find('span'));\n",
       "        console.log(out);\n",
       "        return collectText($(element).find('.CodeMirror-line').find('span'))[1];\n",
       "    });\n",
       "    $(filtered_results).remove();\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$(document).ready(function() {\n",
    "    console.log('Ready');\n",
    "    \n",
    "    \n",
    "    function is_hive_command(list_tokens) {\n",
    "        return list_tokens.indexOf('hive') > -1 && \n",
    "             list_tokens.indexOf('f') > -1 &&\n",
    "             list_tokens.indexOf('-') > -1 && \n",
    "             list_tokens.indexOf('!') > -1 &&\n",
    "             list_tokens.indexOf('hql') > -1 && \n",
    "             list_tokens.indexOf('writefile') == -1;\n",
    "    } \n",
    "    \n",
    "    function collectText(input_tag) {\n",
    "\n",
    "        var result_string = [];\n",
    "        $.each($(input_tag).children(), function(index, child) {\n",
    "            result_string.push($(child).text());\n",
    "        });\n",
    "        return [result_string, is_hive_command(result_string)];\n",
    "    };\n",
    "    \n",
    "    var filtered_results = $(\".cell.code_cell.rendered\").filter(function(index, element) {\n",
    "        var out = collectText($(element).find('.CodeMirror-line').find('span'));\n",
    "        console.log(out);\n",
    "        return collectText($(element).find('.CodeMirror-line').find('span'))[1];\n",
    "    });\n",
    "    $(filtered_results).remove();\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                  \tstring              \t                    \n",
      "year                \tstring              \t                    \n",
      "month               \tstring              \t                    \n",
      "\t \t \n",
      "# Partition Information\t \t \n",
      "# col_name            \tdata_type           \tcomment             \n",
      "\t \t \n",
      "year                \tstring              \t                    \n",
      "month               \tstring              \t                    \n",
      "2008\t2008-10\t73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar]\n",
      "OK\n",
      "Time taken: 0.701 seconds\n",
      "OK\n",
      "Time taken: 0.389 seconds\n",
      "OK\n",
      "Time taken: 0.192 seconds\n",
      "OK\n",
      "Time taken: 0.024 seconds\n",
      "OK\n",
      "Time taken: 1.359 seconds\n",
      "OK\n",
      "Time taken: 0.15 seconds\n",
      "OK\n",
      "Time taken: 0.242 seconds, Fetched: 9 row(s)\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar]\n",
      "OK\n",
      "Time taken: 0.023 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = jovyan_20201215235813_8e798e48-08df-4578-8d44-118e87c5dfe2\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1608075810918_0005, Tracking URL = http://172.17.0.31:8088/proxy/application_1608075810918_0005/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1608075810918_0005\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2020-12-15 23:58:21,249 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-12-15 23:58:36,561 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.62 sec\n",
      "MapReduce Total cumulative CPU time: 10 seconds 140 msec\n",
      "Ended Job = job_1608075810918_0005\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://localhost:9000/user/jovyan/demodb/posts_sample/.hive-staging_hive_2020-12-15_23-58-13_782_4925674735452763922-1/-ext-10000\n",
      "Loading data to table demodb.posts_sample partition (year=null, month=null)\n",
      "\n",
      "\n",
      "\t Time taken to load dynamic partitions: 3.866 seconds\n",
      "\t Time taken for adding to write entity : 0.006 seconds\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 10.14 sec   HDFS Read: 60004098 HDFS Write: 347300 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 10 seconds 140 msec\n",
      "OK\n",
      "Time taken: 63.701 seconds\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.021 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = jovyan_20201215235917_9eb9d36c-0a6a-4bb8-916d-99b0043279f2\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1608075810918_0006, Tracking URL = http://172.17.0.31:8088/proxy/application_1608075810918_0006/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1608075810918_0006\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-12-15 23:59:24,933 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-12-15 23:59:30,038 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.71 sec\n",
      "2020-12-15 23:59:34,096 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.63 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 630 msec\n",
      "Ended Job = job_1608075810918_0006\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1608075810918_0007, Tracking URL = http://172.17.0.31:8088/proxy/application_1608075810918_0007/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1608075810918_0007\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-12-15 23:59:44,664 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-12-15 23:59:48,734 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.85 sec\n",
      "2020-12-15 23:59:53,825 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.48 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 480 msec\n",
      "Ended Job = job_1608075810918_0007\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.63 sec   HDFS Read: 386273 HDFS Write: 2903 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.48 sec   HDFS Read: 11960 HDFS Write: 115 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 110 msec\n",
      "OK\n",
      "Time taken: 38.378 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hive -f task1.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have completed the assignment! Now you can submit it to the system and get your results!\n",
    "\n",
    "Copy your notebook from the steps <a href=\"#Step-4.-Complete-the-assignment\">Step 4</a> and <a href=\"#Step-5.-Submission-part.-Do-not-touch!!-And-simple-run-all-cells-below!\">Step 5</a> to the new notebook. Run all the cells! And submit the copied notebook!\n",
    "\n",
    "Please take into account that the grading system catch all output (both result and MapReduce logs) from the last cell of the notebook, so __don't__ redirect any output from this cell to `/dev/null`\n",
    "\n",
    "#### Final notice:\n",
    "\n",
    "1. Please take into account that you must __not__ redirect __stderr__ to anywhere. Hadoop, Hive, and Spark print their logs to stderr and the Grading system also reads and analyses it.\n",
    "\n",
    "1. During checking the code from the notebook, the system runs all notebook's cells and reads the output of only the last filled cell. It is clear that any exception should not be thrown in the running cells. If you decide to write some text in a cell, you should change the style of the cell to Markdown (Cell -> Cell type -> Markdown).\n",
    "\n",
    "1. The Grader takes into account the output from the sample dataset you have in the notebook. Therefore, you have to \"Run All\" cells in the notebook before you send the ipynb solution.\n",
    "\n",
    "1. The name of the notebook must contain only Roman letters, numbers and characters “-” or “_”. For example, Windows adds something like \" (2)\" (with the leading space) at the end of a filename if you try to download a file with the same name. This is a problem, because you will have a space character and curly braces \"(\" and \")\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
