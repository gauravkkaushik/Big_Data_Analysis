{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive Assignment 3. DML: Calculate Amount of Posts per User Age\n",
    "Use Hive to complete the following task. Input tables was described in Hive Task1 and Hive Task2.\n",
    "\n",
    "Calculate number of questions and answers depending on users' age. Use Ð°ge from 'users' table, filter out users if their age is undefined. Output format:\n",
    "\n",
    "age <tab> number of questions <tab> number of answers\n",
    "    \n",
    "    \n",
    "Example: \n",
    "22 12345 5678\n",
    "\n",
    "\n",
    "\n",
    "Output all ages. Order by age, increment. \n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "\n",
    "...\n",
    "1. 21  11  24\n",
    "2. 22  6   18\n",
    "3. 23  12  15\n",
    "4. 24  16  27\n",
    "5. 25  20  33\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile query_test.hql\n",
    "\n",
    "# use stackoverflow_; \n",
    "# describe posts ;\n",
    "# select * from posts limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hive -f query_test.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting w2t3.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile w2t3.hql\n",
    "\n",
    "use stackoverflow_; \n",
    "\n",
    "select users.age, sum(if(post_type_id = 1, 1, 0)) as n_question, sum(if(post_type_id = 2, 1, 0)) as n_answer\n",
    "from posts, users\n",
    "where posts.owner_user_id = users.id and users.age is not null\n",
    "group by users.age\n",
    "order by users.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 4.037 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = jovyan_20201230185945_9ae37d73-2c13-4153-8dfd-35b31be5c246\n",
      "Total jobs = 2\n",
      "2020-12-30 18:59:52\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2020-12-30 18:59:53\tDump the side-table for tag: 1 with group count: 5951 into file: file:/tmp/jovyan/2601b21e-4967-4146-b2c7-cc559e4cf6dc/hive_2020-12-30_18-59-45_144_8483611401033681139-1/-local-10006/HashTable-Stage-2/MapJoin-mapfile01--.hashtable\n",
      "2020-12-30 18:59:53\tUploaded 1 File to: file:/tmp/jovyan/2601b21e-4967-4146-b2c7-cc559e4cf6dc/hive_2020-12-30_18-59-45_144_8483611401033681139-1/-local-10006/HashTable-Stage-2/MapJoin-mapfile01--.hashtable (137665 bytes)\n",
      "2020-12-30 18:59:53\tEnd of local task; Time Taken: 1.38 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1609354483607_0001, Tracking URL = http://172.17.0.79:8088/proxy/application_1609354483607_0001/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1609354483607_0001\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-12-30 19:00:04,673 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-12-30 19:00:12,873 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 5.76 sec\n",
      "2020-12-30 19:00:17,989 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.07 sec\n",
      "MapReduce Total cumulative CPU time: 7 seconds 70 msec\n",
      "Ended Job = job_1609354483607_0001\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1609354483607_0002, Tracking URL = http://172.17.0.79:8088/proxy/application_1609354483607_0002/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1609354483607_0002\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2020-12-30 19:00:28,776 Stage-3 map = 0%,  reduce = 0%\n",
      "2020-12-30 19:00:32,907 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 0.92 sec\n",
      "2020-12-30 19:00:38,054 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.07 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 70 msec\n",
      "Ended Job = job_1609354483607_0002\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.07 sec   HDFS Read: 2431656 HDFS Write: 1056 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 2.07 sec   HDFS Read: 6991 HDFS Write: 1039 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 9 seconds 140 msec\n",
      "OK\n",
      "14\t1\t0\n",
      "15\t1\t2\n",
      "16\t2\t0\n",
      "17\t0\t1\n",
      "18\t4\t1\n",
      "19\t1\t1\n",
      "20\t0\t2\n",
      "21\t11\t24\n",
      "22\t6\t18\n",
      "23\t12\t15\n",
      "24\t16\t27\n",
      "25\t20\t33\n",
      "26\t23\t44\n",
      "27\t28\t56\n",
      "28\t24\t37\n",
      "29\t24\t66\n",
      "30\t26\t67\n",
      "31\t17\t33\n",
      "32\t13\t48\n",
      "33\t11\t40\n",
      "34\t24\t36\n",
      "35\t12\t42\n",
      "36\t8\t64\n",
      "37\t9\t35\n",
      "38\t6\t17\n",
      "39\t3\t7\n",
      "40\t1\t13\n",
      "41\t5\t20\n",
      "42\t5\t22\n",
      "43\t2\t26\n",
      "44\t7\t35\n",
      "45\t1\t4\n",
      "46\t7\t9\n",
      "47\t1\t1\n",
      "48\t1\t1\n",
      "49\t1\t26\n",
      "50\t1\t26\n",
      "51\t4\t5\n",
      "52\t0\t2\n",
      "53\t0\t2\n",
      "54\t0\t1\n",
      "57\t0\t3\n",
      "58\t1\t57\n",
      "60\t0\t6\n",
      "61\t0\t3\n",
      "64\t2\t1\n",
      "86\t0\t1\n",
      "96\t3\t1\n",
      "Time taken: 53.963 seconds, Fetched: 48 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f w2t3.hql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
