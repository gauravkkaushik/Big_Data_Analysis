{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common warnings:\n",
    "\n",
    "1. __Backup your solution into the 'work' directory inside the home directory ('/home/jovyan'). It is the only one that state will be saved over sessions.__\n",
    "\n",
    "1. Please, ensure that you call the right interpreter (python2 or python3). Do not write just \"python\" without the major version. There is no guarantee that any particular version of Python is set as the default one in the Grading system.\n",
    "\n",
    "1. One cell must contain only one programming language.\n",
    "E.g. if a cell contains Python code and you also want to call a bash-command (using “!”) in it, you should move the bash to another cell.\n",
    "\n",
    "1. Our IPython converter is an improved version of the standard converter Nbconvert and it can process most of Jupyter's magic commands correctly (e.g. it understands \"%%bash\" and executes the cell as a \"bash\"-script). However, we highly recommend to avoid magics wherever possible.\n",
    "\n",
    "#### Spark specific warnings:\n",
    "\n",
    "1. It is a good practice to run Spark with master \"yarn\". However, containered system's performance is limited. If you see repeating Py4JavaErrors or Py4JNetworkErrors exceptions which you assume are not relevant to your code, feel free to change master to “local”.\n",
    "\n",
    "1. You should eliminate extra symbols in output (such as quotes, brackets etc.). When you finally get the resulting dataframe it is easier to print wiki.take(1) instead of traverse RDD using for cycle. But in this case a lot of junk symbols will be printed like: `[['Anarchism', 'is', .. ]]`. See the right output example in the task.\n",
    "\n",
    "#### Task hint\n",
    "Each subsequent of these tasks is a continuation of the previous one. So, you may use the same IPython notebook for all the programming assignments in this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark_session = SparkSession.builder.enableHiveSupport().master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark_session.read.parquet(\"/data/sample264\")\n",
    "meta = spark_session.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+----------+\n",
      "|userId|trackId|artistId| timestamp|\n",
      "+------+-------+--------+----------+\n",
      "| 13065| 944906|  978428|1501588527|\n",
      "|101897| 799685|  989262|1501555608|\n",
      "|215049| 871513|  988199|1501604269|\n",
      "|309769| 857670|  987809|1501540265|\n",
      "|397833| 903510|  994595|1501597615|\n",
      "|501769| 818149|  994975|1501577955|\n",
      "|601353| 958990|  973098|1501602467|\n",
      "|710921| 916226|  972031|1501611582|\n",
      "|  6743| 801006|  994339|1501584964|\n",
      "|152407| 913509|  994334|1501571055|\n",
      "+------+-------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider joining the graph to itself with the UserId remove pairs with the same tracks\n",
    "For each track choose top 50 tracks ordered by weight similar to it and normalize weights of its edges (divide the weight of each edge on a sum of weights of all edges).\n",
    "Use rank() to choose top 40 tracks as is done in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the garph with itself\n",
    "# take diffrence of timestamp and apply filter for listing within 7 minutes\n",
    "# Remove pairs with same tracks\n",
    "filtered_df = (data.alias('temp_df1').join(data.alias('temp_df2'), 'userId')\n",
    "               .withColumn('timestamp_diff', f.col('temp_df1.timestamp') - f.col('temp_df2.timestamp'))\n",
    "               .where((f.col('timestamp_diff') > 0) \n",
    "                      & (f.col('timestamp_diff') <= 420) \n",
    "                      & (f.col('temp_df1.trackId') != f.col('temp_df2.trackId')))\n",
    "              )\n",
    "\n",
    "tracks = (filtered_df.select(f.col('temp_df1.trackId').alias('track1'), f.col('temp_df2.trackId').alias('track2'))\n",
    "          .withColumn('id1', f.when((f.col('track1') < f.col('track2')), f.col('track1')).otherwise(f.col('track2')))\n",
    "          .withColumn('id2', f.when((f.col('track1') < f.col('track2')), f.col('track2')).otherwise(f.col('track1')))\n",
    "          .select('id1', 'id2')\n",
    "          .groupBy(f.col('id1'), f.col('id2')).count()\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum,col\n",
    "\n",
    "# normalize weights of its edges (divide the weight of each edge on a sum of weights of all edges).\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tracks = norm(tracks, 'id1', 'id2', 'count', 40).select('id1', 'id2', 'norm_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id1=798477, id2=883244, norm_count=1.0),\n",
       " Row(id1=798692, id2=898823, norm_count=1.0),\n",
       " Row(id1=800467, id2=855206, norm_count=1.0),\n",
       " Row(id1=801701, id2=920990, norm_count=1.0),\n",
       " Row(id1=802599, id2=908754, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=937714, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=811513, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=929402, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=924227, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=901687, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=860294, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=880642, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=920627, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=843219, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=892457, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=823001, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=899859, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=866435, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=881358, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=901328, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=955459, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=949099, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=813969, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=852638, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=817399, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=960498, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=814352, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=894214, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=909544, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=860406, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=888497, norm_count=0.03571428571428571),\n",
       " Row(id1=802599, id2=962724, norm_count=0.03571428571428571),\n",
       " Row(id1=803868, id2=862399, norm_count=0.5),\n",
       " Row(id1=803868, id2=821251, norm_count=0.5),\n",
       " Row(id1=808110, id2=894437, norm_count=1.0),\n",
       " Row(id1=808445, id2=902586, norm_count=0.2916666666666667),\n",
       " Row(id1=808445, id2=833060, norm_count=0.2916666666666667),\n",
       " Row(id1=808445, id2=890834, norm_count=0.08333333333333333),\n",
       " Row(id1=808445, id2=827143, norm_count=0.08333333333333333),\n",
       " Row(id1=808445, id2=935934, norm_count=0.041666666666666664),\n",
       " Row(id1=808445, id2=844790, norm_count=0.041666666666666664),\n",
       " Row(id1=808445, id2=892808, norm_count=0.041666666666666664),\n",
       " Row(id1=808445, id2=881466, norm_count=0.041666666666666664),\n",
       " Row(id1=808445, id2=826414, norm_count=0.041666666666666664),\n",
       " Row(id1=808445, id2=928370, norm_count=0.041666666666666664),\n",
       " Row(id1=809289, id2=847119, norm_count=1.0),\n",
       " Row(id1=814446, id2=870227, norm_count=1.0),\n",
       " Row(id1=814618, id2=829417, norm_count=0.25),\n",
       " Row(id1=814618, id2=886044, norm_count=0.25),\n",
       " Row(id1=814618, id2=940951, norm_count=0.25)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tracks.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(f.col('norm_count').desc())\n",
    "    \n",
    "TrackList = normalized_tracks.withColumn('position', f.rank().over(window)) \\\n",
    "    .filter(f.col('position') < 40) \\\n",
    "    .orderBy(f.col('id1'), f.col('id2')) \\\n",
    "    .select('id1', 'id2') \\\n",
    "    .take(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798256 923706\n",
      "798319 837992\n",
      "798322 876562\n",
      "798331 827364\n",
      "798335 840741\n",
      "798374 816874\n",
      "798375 810685\n",
      "798379 812055\n",
      "798380 840113\n",
      "798396 817687\n",
      "798398 926302\n",
      "798405 867217\n",
      "798443 905923\n",
      "798457 918918\n",
      "798460 891840\n",
      "798461 940379\n",
      "798470 840814\n",
      "798474 963162\n",
      "798477 883244\n",
      "798485 955521\n",
      "798505 905671\n",
      "798545 949238\n",
      "798550 936295\n",
      "798626 845438\n",
      "798691 818279\n",
      "798692 898823\n",
      "798702 811440\n",
      "798704 937570\n",
      "798725 933147\n",
      "798738 894170\n",
      "798745 799665\n",
      "798782 956938\n",
      "798801 950802\n",
      "798820 890393\n",
      "798833 916319\n",
      "798865 962662\n",
      "798931 893574\n",
      "798946 946408\n",
      "799012 809997\n",
      "799024 935246\n"
     ]
    }
   ],
   "source": [
    "for val in TrackList:\n",
    "    print(\"%s %s\" % val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
